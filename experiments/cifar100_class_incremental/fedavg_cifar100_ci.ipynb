{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting is the federated class incremental settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the train parameters.\n",
    "from configs import configs\n",
    "configs['test_batchsize']=128\n",
    "configs['test_dataset_size']=1000\n",
    "configs['train_dataset_size']=5000\n",
    "configs['head_finetune_epoch']=2\n",
    "configs['train_epoch']=5\n",
    "configs['head_ft_learn_rate']=0.001\n",
    "configs['learn_rate']=0.001\n",
    "configs['train_batchsize']=128\n",
    "\n",
    "number_of_clients=10\n",
    "total_rounds=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "from fcl_data_simulator.single_dataset import CIFAR100\n",
    "cifar100=CIFAR100(resize_to=32)\n",
    "trainset=cifar100[\"train\"]\n",
    "testset=cifar100[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcl_data_simulator.dataset_utils import get_index_by_class\n",
    "from fcl_data_simulator.dataset_utils import create_sampled_dataset\n",
    "\n",
    "train_idx=get_index_by_class(trainset)\n",
    "test_idx=get_index_by_class(testset)\n",
    "class_idx=[list(range(0,25)),list(range(25,50)),\n",
    "           list(range(50,75)),list(range(75,100))]\n",
    "train_tasks=[]\n",
    "test_tasks=[]\n",
    "for task_class_idx in class_idx:\n",
    "    train_task_idx=[]\n",
    "    test_task_idx=[]\n",
    "    for class_index in task_class_idx:\n",
    "        train_task_idx+=train_idx[class_index]\n",
    "        test_task_idx+=test_idx[class_index]\n",
    "    train_tasks.append(create_sampled_dataset(trainset,train_task_idx,True))\n",
    "    test_tasks.append(create_sampled_dataset(testset,test_task_idx,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcl_data_simulator.continual_policy import TaskSeparateContiunalPolicy\n",
    "# Set the continual policies.\n",
    "train_cp=TaskSeparateContiunalPolicy(train_tasks,25,\n",
    "            configs['train_dataset_size'])\n",
    "test_cp=TaskSeparateContiunalPolicy(test_tasks,25,\n",
    "            configs['test_dataset_size'])\n",
    "\n",
    "# Set the partition policy for the train dataset.\n",
    "from fcl_data_simulator.partition_policy import PartitioningPolicies\n",
    "partition_policy=PartitioningPolicies.dirichlet_nonIID_partitioning\n",
    "partition_policy_args={\"number_of_clients\":number_of_clients}\n",
    "test_partition_policy_args={\"number_of_clients\":1}\n",
    "\n",
    "# Create the DataManager.\n",
    "from fcl_data_simulator.data_manager import DataManager\n",
    "train_data_manager=DataManager(configs[\"train_batchsize\"],train_cp,\n",
    "                    partition_policy,partition_policy_args)\n",
    "test_data_manager=DataManager(configs[\"test_batchsize\"],test_cp,\n",
    "                    partition_policy,test_partition_policy_args)\n",
    "\n",
    "print(\"The fcl data simulator is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Set the model.\n",
    "shared_model=torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 6, 5),\n",
    "    torch.nn.ReLU(),torch.nn.MaxPool2d(2,2),\n",
    "    torch.nn.Conv2d(6, 16, 5),\n",
    "    torch.nn.ReLU(),torch.nn.MaxPool2d(2,2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(400, 200),\n",
    ")\n",
    "# The head library.\n",
    "head_library={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.fcl_fedavg_client import fcl_fedavg_client\n",
    "# Set the clients.\n",
    "clients=[fcl_fedavg_client() for _ in range(number_of_clients)]\n",
    "# The client used to test.\n",
    "pseudo_client=fcl_fedavg_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation\n",
    "from fcl_server import fcl_server\n",
    "from fcl_data_simulator.dataset_utils import sample_slice\n",
    "# Now the train process.\n",
    "\n",
    "client_accuracies=[]\n",
    "'''The accuracy of clients over rounds.'''\n",
    "server_accuracy=[]\n",
    "'''The test accuracy of the aggregated model over rounds.'''\n",
    "task_accuracies=[]\n",
    "'''The test accuracy of the aggregated model on visited tasks.'''\n",
    "\n",
    "for round in range(total_rounds):\n",
    "    client_weight=[]\n",
    "    client_model=[]\n",
    "    client_head=[]\n",
    "    client_acc=[]\n",
    "    for client_idx,client in enumerate(clients):\n",
    "        client.update_model(shared_model,head_library)\n",
    "        client.head_need_ft=False # The head_library is shared so don't need ft.\n",
    "\n",
    "        train_slice,slice_classes=train_data_manager.get_slice(client_idx)\n",
    "\n",
    "        client_weight.append(len(train_slice.dataset))\n",
    "\n",
    "        acc=client.train_model(train_slice,slice_classes,head_ft=(round!=0))\n",
    "        \n",
    "        client_acc.append(acc)\n",
    "        \n",
    "        model,head=client.get_model()\n",
    "        \n",
    "        client_model.append(model)\n",
    "        client_head.append(head)\n",
    "    \n",
    "    print(\"Round {}\".format(round))\n",
    "    print(\"\\tClient accuracies:\",client_acc)\n",
    "    client_accuracies.append(client_acc)\n",
    "    shared_model=fcl_server.aggregate_shared_model(client_model,client_weight)\n",
    "    head_library=fcl_server.aggregate_heads(client_head,client_weight)\n",
    "\n",
    "    # Now do the test.\n",
    "    pseudo_client.update_model(shared_model,head_library)\n",
    "    test_slice,test_slice_classes=test_data_manager.get_slice(0)\n",
    "    test_acc=pseudo_client.test_model(test_slice,test_slice_classes)\n",
    "    print(\"\\tTest accuracy:\",test_acc)\n",
    "    server_accuracy.append(test_acc)\n",
    "\n",
    "    # Evaluate on past tasks.\n",
    "    past_tasks=test_data_manager.get_past_tasks()\n",
    "    past_task_acc=[]\n",
    "    for past_task in past_tasks:\n",
    "        ptask_slice,ptask_classes=sample_slice(past_task,\n",
    "                                    configs['test_dataset_size'], \n",
    "                                    configs['test_batchsize'])\n",
    "        ptask_acc=pseudo_client.test_model(ptask_slice,ptask_classes)\n",
    "        past_task_acc.append(ptask_acc)\n",
    "    \n",
    "    task_accuracies.append(past_task_acc)\n",
    "\n",
    "    # Step the data manager until the last round.\n",
    "    if round!=total_rounds-1:\n",
    "        train_data_manager.next_round()\n",
    "        test_data_manager.next_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now dump the accuracy for future use.\n",
    "import json\n",
    "jsonstr=json.dumps({\"client_acc\":client_accuracies,\"server_acc\":server_accuracy,\"task\":task_accuracies})\n",
    "print(jsonstr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3311224cefd6618c2e820ef9ecbdd553eb84ddf2e1c75c922a927e32a989153"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('fed_ltw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
